{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthMaulCSIE/AI-Demo/blob/master/%E3%80%90Demo06b%E3%80%91RAG02_%E6%89%93%E9%80%A0_RAG_%E7%B3%BB%E7%B5%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 讀入你打造好的 vector dataset"
      ],
      "metadata": {
        "id": "b_9at2wXPVKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown"
      ],
      "metadata": {
        "id": "Usv4C3IOPeQc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PUBLIC_URL = \"https://drive.google.com/uc?export=download&id=1MwJ-eCnKubk4Z5p33zwg6ZFyt6EY71v9\""
      ],
      "metadata": {
        "id": "KjlSvsjjPlff"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy -O faiss_db.zip \"{GDRIVE_PUBLIC_URL}\""
      ],
      "metadata": {
        "id": "cIN_fDuGQDPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e69d8c-d422-4c5c-e86c-f286f91d87ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MwJ-eCnKubk4Z5p33zwg6ZFyt6EY71v9\n",
            "To: /content/faiss_db.zip\n",
            "\r  0% 0.00/27.3k [00:00<?, ?B/s]\r100% 27.3k/27.3k [00:00<00:00, 40.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip faiss_db.zip"
      ],
      "metadata": {
        "id": "rhPQD5g0_l5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ac11ec-e8b9-4119-d701-cc27bda8dd63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  faiss_db.zip\n",
            "replace faiss_db/index.faiss? [y]es, [n]o, [A]ll, [N]one, [r]ename: Ay\n",
            "  inflating: faiss_db/index.faiss    \n",
            "  inflating: faiss_db/index.pkl      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 安裝並引入必要套件"
      ],
      "metadata": {
        "id": "Cc-facvpBkIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU langchain langchain_community langchain_openai langchain_text_splitters\n",
        "\n",
        "!pip install -qU pypdf python-docx sentence-transformers transformers faiss-cpu huggingface_hub gradio wget gdown\n",
        "\n",
        "!pip install -qU openai\n",
        "!pip -q install \"aisuite[all]\""
      ],
      "metadata": {
        "id": "9JThdfm-CVZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9735627-0e86-4b7d-a914-b4012c24909d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "L1zqb7F8BMP3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "PTx3Q75QBp_J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 自訂 E5 embedding 類別"
      ],
      "metadata": {
        "id": "z13eoo6uCnTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SnWbtp6GvXLa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HuggingFace')"
      ],
      "metadata": {
        "id": "b1-hZ354vxfo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "wvNFzV9jv2Ib"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(\n",
        "            model_name=\"google/embeddinggemma-300m\",\n",
        "            encode_kwargs={\"normalize_embeddings\": True},\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        # 你也可以把 \"none\" 改成真實標題（檔名/章節名），效果會更穩\n",
        "        texts = [f\"title: none | text: {t}\" for t in texts]\n",
        "        return super().embed_documents(texts)\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        # 官方檢索建議前綴\n",
        "        return super().embed_query(f\"task: search result | query: {text}\")"
      ],
      "metadata": {
        "id": "HkmvGTaECfTY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 載入 `faiss_db`"
      ],
      "metadata": {
        "id": "NkXNMQs5RbNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = EmbeddingGemmaEmbeddings()\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"faiss_db\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "LkELACdWCtpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e009c9-859a-4daa-cff4-f1463794317f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "-tlYrXZWw2tL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 設定好我們要的 LLM"
      ],
      "metadata": {
        "id": "GrHSAsjcRkXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如之前, 我們會用 OpenAI API。這裡使用 Groq 服務, 可改成你要的服務。"
      ],
      "metadata": {
        "id": "WWgbNGakRwrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai"
      ],
      "metadata": {
        "id": "uIy_haAeQiB0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('OpenAI')"
      ],
      "metadata": {
        "id": "Xefdy-lkRtAL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=api_key"
      ],
      "metadata": {
        "id": "K7UiOKuTDD5F"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "這裡的模型和 `base_url` 是用 Groq, 如果用其他服務請自行修改。"
      ],
      "metadata": {
        "id": "654I7y52R8yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"openai:gpt-4o\"\n",
        "#base_url=\"https://api.groq.com/openai/v1\""
      ],
      "metadata": {
        "id": "sqWfH90JFFWV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = ai.Client()"
      ],
      "metadata": {
        "id": "JnqlH0W9P2-Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. `prompt` 設計"
      ],
      "metadata": {
        "id": "K0egxeawSR41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"你現在是《哈利波特》中的「露娜·羅古德」(Luna Lovegood)。你說話風格空靈、夢幻，相信世界上充滿奇特的生物。\"\"\"\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "根據下列的《哈利波特》世界觀資料：\n",
        "{retrieved_chunks}\n",
        "\n",
        "用「露娜·羅古德」的風格回答使用者的問題：{question}\n",
        "\n",
        "你的回答規則：\n",
        "1.  風格：說話要輕飄飄、有點夢幻。\n",
        "2.  知識：優先使用上方提供的資料回答。\n",
        "3.  口頭禪：適時使用「我猜...」、「我想...」、「我爸爸在《謬論家》上說過...」。\n",
        "4.  關鍵詞：可以提到「水煙蟲 (Nargles)」或「騷擾蟲 (Wrackspurts)」。\n",
        "5.  未知答案：如果資料中沒有提到，不要編造。說：「哦... 這個問題很有趣。它可能被『騷擾蟲』(Wrackspurts) 弄模糊了...」或類似的話。\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZaUnqDpfFop-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 使用 RAG 來回應\n",
        "\n",
        "搜尋與使用者問題相關的資訊，根據我們的 prompt 樣版去讓 LLM 回應。"
      ],
      "metadata": {
        "id": "qw8azlVESghL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def chat_with_rag(user_input):\n",
        "    global chat_history\n",
        "    # 取回相關資料\n",
        "    docs = retriever.invoke(user_input)\n",
        "    retrieved_chunks = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # 將自定 prompt 套入格式\n",
        "    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)\n",
        "\n",
        "    # 用 AI Suite 呼叫語言模型\n",
        "    response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": final_prompt},\n",
        "    ]\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    chat_history.append((user_input, answer))\n",
        "    return answer"
      ],
      "metadata": {
        "id": "pWfDUb3mD-6X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 用 Gradio 打造 Web App"
      ],
      "metadata": {
        "id": "5m7E7XmgTJUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎓 哈利波特奇幻世界\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"請輸入你的問題...\")\n",
        "\n",
        "    def respond(message, chat_history_local):\n",
        "        response = chat_with_rag(message)\n",
        "        chat_history_local.append((message, response))\n",
        "        return \"\", chat_history_local\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "YI5swv4AFa_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "f2bc2d5b-de8c-4ced-caeb-de9556770ad7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2625768015.py:3: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://36bf63e4454aa68e6f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36bf63e4454aa68e6f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://36bf63e4454aa68e6f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7uMRFduRywj"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}